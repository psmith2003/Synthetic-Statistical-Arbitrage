---
title: "Synthetic Statistical Arbitrage"
author: "Parker Smith"
format: 
  html:
    theme: darkly
    code-fold: true
execute:
  echo: true
  warning: false
  error: false
editor_options: 
  chunk_output_type: console
---


::: {.panel-tabset}

## Overview
The foundation of this strategy is that traders assume similar securities will move together. For example, consider two securities, whose prices are denoted as $S_B$ and $S_P$. If $S_B$ or $S_P$ deviates from the other, we expect them to converge. This phenomenon can be exploited by buying one security and shorting the other. The following strategy will calculate which securities to short, which to buy, and in what quantities. It will also allow for more securities to be included, which can improve model performance.


## Formulation
### Relating $S_B$ and $S_P$
To describe the phenomenon explained in the overview, consider two securities denoted $S_B$ and $S_P$, where $S_B$ is the "base security" and $S_P$ is the "pseudo-security".  
Let $$S_{B,t} = S_{P,t} + \phi_t$$
where $\phi_t \in \mathbb{R}$.  
While the base security ($S_B$) is just a single asset, the pseudo-security ($S_P$) is more complex. It is a linear combination of securities that follows $S_B$. Thus, $S_P$ is a synthetic asset built from other securities. By substituting $S_{P,t} = \sum_{i=1}^{n} \gamma_i S_{i,t}$ we get: $$S_{B,t} = \sum_{i=1}^{n} \gamma_i S_{i,t} + \phi_t$$  
where $\gamma_i \in \mathbb{R}$.  
All $\gamma_i$ can be estimated using various methods. Here, least squares will be used, though gradient descent could give a less overfit model.


### How to trade $\phi$
Using the formula $S_{B,t} = \sum_{i=1}^{n} \gamma_i S_{i,t} + \phi_t$, solving for $\phi_t$ gives: $$\phi_t = S_{B,t} - \sum_{i=1}^{n} \gamma_i S_{i,t}$$
The $\gamma_i$ provides weights for each security to be bought, with a weight of $1$ for $S_B$. Each security $S_i$ will have $\gamma_i$ shares bought. A negative $\gamma_i$ indicates a short position.

### Properties of $\phi$
```{r}
library(tidyverse)
library(forecast)
library(knitr)
library(kableExtra)
library(scales)
library(purrr)

# Folder containing all your CSVs
folder_path <- "data\\crypto" # this can be any other collection of securities

# Get a list of all CSV files in the folder
files <- list.files(folder_path, pattern = "\\.csv$", full.names = TRUE)

# Read all CSVs and combine them into one data frame
exclude = c('USDT', 'USDC', 'WBTC') # removed any constant or 1-to-1 coins
securities_data <- files %>%
  lapply(read.csv) %>%
  bind_rows() %>% 
  filter(!Symbol %in% exclude)

securities_wide <- securities_data %>%
  select(Symbol, Date, Close) %>%          # Keep only relevant columns
  pivot_wider(
    names_from = Symbol,                   # Column names come from the coin symbol
    values_from = Close                    # Values come from the Close column
  ) %>%
  arrange(Date) %>% 
  drop_na()

securities_wide$Date = as.Date(securities_wide$Date)

n <- nrow(securities_wide)
split <- floor(0.8 * n)   # 80% train

train <- securities_wide[1:split, ]
test  <- securities_wide[(split+1):n, ]


auto_lm <- function(df, response, alpha = 0.05) {
  # start with all predictors except response and Date
  predictors <- setdiff(names(df), c(response, "Date"))
  
  formula <- as.formula(paste(response, "~", paste(predictors, collapse = " + "), "+ 0"))
  model <- lm(formula, data = df)
  
  repeat {
    pvals <- summary(model)$coefficients[, 4]  # no intercept anymore
    max_p <- max(pvals, na.rm = TRUE)
    
    if (max_p < alpha) break  # stop if all are significant
    
    worst_var <- names(which.max(pvals))
    predictors <- setdiff(predictors, worst_var)
    
    formula <- as.formula(paste(response, "~", paste(predictors, collapse = " + "), "+ 0"))
    model <- lm(formula, data = df)
  }
  
  return(model)
}

model = auto_lm(securities_wide[ , !(names(securities_wide) %in% "Date")], 'BTC', alpha = 1)

fit = Arima(model$residuals, order = c(1, 0, 0), include.mean = F)
```
To examine $\phi$, data from 23 cryptocurrencies will be used. The data is from 2020-10-05 to 2021-07-06. The following is $\phi$ from a base of BTC.

```{r}
theme_dark_mode <- function() {
  theme_minimal(base_family = "sans") +
    theme(
      plot.background = element_rect(fill = "#1e1e1e", color = NA),
      panel.background = element_rect(fill = "#1e1e1e", color = NA),
      panel.grid.major = element_line(color = "#444444"),
      panel.grid.minor = element_line(color = "#242424"),
      axis.text = element_text(color = "gray80"),
      axis.title = element_text(color = "gray80"),
      plot.title = element_text(color = "gray80", face = "bold", size = 14),
      plot.subtitle = element_text(color = "gray80", size = 12),
      legend.background = element_rect(fill = "#1e1e1e", color = NA),
      legend.text = element_text(color = "gray80"),
      legend.title = element_text(color = "gray80", face = "bold")
    )
}


phi = data.frame(phi = model$residuals,
                 Date = securities_wide$Date)

ggplot(phi, aes(y=phi, x=Date)) +
  geom_line(color="#999999", linewidth=.75) +
  geom_hline(yintercept = 0, color='blue') +
  theme_dark_mode() +
  labs(
    title = expression(phi * " from 2020-10-05 to 2021-07-06"),
    subtitle = "BTC Base",
    y = expression(phi~"($)"),
    x = NULL
  ) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b %Y") +
  scale_y_continuous(labels = scales::dollar)

  
```
$\phi_t$ is essentially the residual from the relationship $S_{B,t} = S_{P,t}$. If $S_P$ follows $S_B$ and they are thought to converge, then $\phi$ should move toward $0$, which is seen in the chart above.  This suggests an autoregressive (AR) model. After testing various orders, the AR(1) model provided the best AIC, avoiding overfitting. This gives a new equation for $\phi_t$:

$$
\phi_t = \lambda \phi_{t-1} + \sigma z_t
$$
where $\lambda,\sigma \in \mathbb{R}$ and $z_t$ is a standard normal variable.  

An AR(1) model can be fit to $\phi$. This model will use all 23 cryptocurrencies in the dataset.
```{r}

# Simple data frame from ARIMA(1,0,0) model results
fit <- arima(model$residuals, order = c(1, 0, 0), include.mean = FALSE)

# Extract coefficients and standard errors
coefficients <- coef(fit)
std_errors <- sqrt(diag(vcov(fit)))

# Calculate t-values and p-values
t_values <- coefficients / std_errors
p_values <- 2 * (1 - pnorm(abs(t_values)))

# Get standard deviation of the shock (innovation standard deviation)
shock_sd <- sqrt(fit$sigma2)

# Create simple results data frame
results_df <- data.frame(
  Parameter = c(names(coefficients), "shock_sd"),
  Estimate = c(round(coefficients, 4), round(shock_sd, 4))
  # ,
  # Std_Error = c(round(std_errors, 4), NA),
  # t_value = c(round(t_values, 4), NA),
  # p_value = c(round(p_values, 6), NA)
)
results_df$Parameter[results_df$Parameter == "ar1"] <- "\\(\\lambda\\)"
results_df$Parameter[results_df$Parameter == "shock_sd"] <- "\\(\\sigma\\)"

kable(results_df, format = "html", escape = FALSE, row.names = F) %>%
  kable_styling(full_width = FALSE)
```

Using fewer securities gives parameters that reduce predictability of $\phi$. The following is using only Ethereum and Bitcoin.
```{r}
BE_df = securities_wide[, c("ETH", "BTC")]
model_BE = auto_lm(BE_df, 'BTC')

# Simple data frame from ARIMA(1,0,0) model results
fit_BE <- arima(model_BE$residuals, order = c(1, 0, 0), include.mean = FALSE)

# Extract coefficients and standard errors
coefficients_BE <- coef(fit_BE)
std_errors_BE <- sqrt(diag(vcov(fit_BE)))

# Calculate t-values and p-values
t_values_BE <- coefficients_BE / std_errors_BE
p_values_BE <- 2 * (1 - pnorm(abs(t_values_BE)))

# Get standard deviation of the shock (innovation standard deviation)
shock_sd_BE <- sqrt(fit_BE$sigma2)

# Create simple results data frame
results_df_BE <- data.frame(
  Parameter = c(names(coefficients_BE), "shock_sd"),
  Estimate = c(round(coefficients_BE, 4), round(shock_sd_BE, 4))
  # ,
  # Std_Error = c(round(std_errors_BE, 4), NA),
  # t_value = c(round(t_values_BE, 4), NA),
  # p_value = c(round(p_values_BE, 6), NA)
)

# Replace names with LaTeX symbols
results_df_BE$Parameter[results_df_BE$Parameter == "ar1"] <- "\\(\\lambda\\)"
results_df_BE$Parameter[results_df_BE$Parameter == "shock_sd"] <- "\\(\\sigma\\)"

# Render table for HTML
kable(results_df_BE, format = "html", escape = FALSE, row.names = FALSE) %>%
  kable_styling(full_width = FALSE)

```
Though $\sigma$ is similar, the increased $\lambda$ results in reduced trade profits and greater profit variance. This will be proved in the "Expected Returns" section. Because of this, more securities are preferred as it tends to provide lower $\lambda$ and $\sigma$.

### Position Sizing
To size trades, log utility maximization will be used. This helps to prevent ruin. Wealth evolves according to: $$W_{t+1} = W_t (1 + f_t R_{t+1})$$ where $W_{t+1}$ is tomorrow's wealth, $W_t$ is today's wealth, $R_{t+1}$ is tomorrow's return on a trade, and $f_{t}$ is the fraction of wealth used in a trade.  
Using log utility maximization we need to maximize: 
$$
\mathbb{E}\!\Bigl[ \ln\Bigl( 1 + f_{t} R_{t+1} \Bigr) \Bigr]
$$
Substituting 
$$
R_{t+1} = \frac{\phi_{t+1}-\phi_{t}}{C_t}
$$
where $C_{t}$ is the cost of purchasing $\phi_{t}$. We then get: 
$$
\mathbb{E}\!\left[ \ln\!\biggl(1 + f_{t} \frac{\phi_{t+1} - \phi_{t}}{C_{t}}\biggr) \right]
$$

Since $-\infty<\phi<\infty$, the expected value cannot be evaluated analytically. Instead, the second order Taylor approximation can be used. Higher orders can be used for greater accuracy. The new approximation is: 
$$
\begin{align}
\mathbb{E}\!\left[\ln\!\left(1+f_{t} \frac{\phi_{t+1}-\phi_{t}}{C_{t}}\right)\right]
&\approx 
\mathbb{E}\!\left[f_{t} \frac{\phi_{t+1}-\phi_{t}}{C_{t}} 
- \frac{1}{2}\left(f_{t} \frac{\phi_{t+1}-\phi_{t}}{C_{t}}\right)^2\right] \\
&= \frac{f_{t}}{C_{t}}( \lambda-1 ) \phi_{t}
- \frac{f_{t}^2}{2 C_{t}^2} \big[(\lambda-1)^2 \phi_{t}^2+\sigma^2\big] 
\quad \text{using } \phi_{t+1}=\lambda\phi_t+\sigma z_{t+1}
\end{align}
$$
By maximizing with respect to $f_t$ we get our trade size to be:
$$
f_t = -\frac{C_{t}(1-\lambda)\phi_t}{(1-\lambda)^2\phi_t^2+\sigma^2}
$$
where $C_t$ is the cost of purchasing $\phi_t$, $\lambda$ is the AR(1) coefficient, and $\sigma$ is the standard deviation of the noise. Note that a negative fraction of wealth implies a short position. 

### Expected Returns

Using our chosen trade size, we can compute a trade’s expected return on wealth:
$$
\begin{align}
\mathbb{E}\!\left[1 + f_t R_{t+1} \,\middle|\, \phi_t\right] 
&= 1 
- \frac{C_t (1 - \lambda)\,\phi_t}{(1 - \lambda)^2 \phi_t^2 + \sigma^2} 
\cdot \mathbb{E}\!\left[\frac{\phi_{t+1} - \phi_t}{C_t}\right] \\[6pt]
&= 1 
- \frac{(1 - \lambda)\,\phi_t}{(1 - \lambda)^2 \phi_t^2 + \sigma^2} 
\cdot \mathbb{E}\!\left[(\lambda - 1)\,\phi_t + \sigma z_t\right] \\[6pt]
&= 1 
+ \frac{(1 - \lambda)^2 \phi_t^2}{(1 - \lambda)^2 \phi_t^2 + \sigma^2}
\end{align}
$$
Since $(1-\lambda)$, $\phi_t$, and $\sigma$ are all squared in this equation, the result is always positive. Thus, the expected return is positive, and the trading strategy will, on average, be profitable.   
  
To show the earlier claim that low $\lambda$ is better for trading, consider the derivative of the expected returns:  

\begin{align}
\frac{d}{d \lambda} \mathbb{E}\!\left[1 + f_t R_{t+1} \,\middle|\, \phi_t\right] 
&= \frac{d}{d \lambda} \left( 
1 + \frac{(1 - \lambda)^2 \phi_t^2}{(1 - \lambda)^2 \phi_t^2 + \sigma^2}
\right) \\
&= \frac{-2(1-\lambda)\,\phi_t^2\,\sigma^2}{\left((1-\lambda)^2 \phi_t^2 + \sigma^2\right)^2}
\end{align}

Since the denominator, $\phi_t$, and $\sigma$ are all squared and $\lambda<1$ we know this derivative will be negative. This means that as $\lambda$ decreases our expected returns increase, thus low values of $\lambda$ lead to higher returns on average.  
  
All equations used in this section are valid only under the assumptions that:

- $\phi$ is an AR(1) process
- $\phi$ continues to follow the same AR(1) process beyond the training data


## Implementation
This section will explore the performance of the trading strategy against real data.  
The trade-sizing equation derived in the Formulation section can over leverage, therefore in all simulations the fraction is divided by $20$ and capped at $\pm0.9$. This reduces risk and safeguards against outliers.
This test also excludes less significant securities from the pseudo-security in an attempt to reduce overfitting.  
Methods of avoiding over-leveraging and overfitting are not rigorous, but heuristically chosen.
  
### Training Data
First, to ensure the mathematics behind the strategy are correct, trading will be simulated on the training data. This is to ensure the underlying theory is consistent.  
```{r}

run_strategy <- function(ticker, data) {
  trading_df <- data %>% select(-Date)
  
  starting_money <- 1000
  balance <- c(starting_money)
  phi_t <- numeric()
  
  model <- auto_lm(trading_df, ticker, 1.00)
  fit <- Arima(model$residuals, order = c(1, 0, 0), include.mean = FALSE)
  
  sigma_2 <- as.numeric(fit$sigma2)
  lambda <- as.numeric(fit$coef)
  sec_coef <- -model$coefficients
  sec_coef[[ticker]] <- 1
  
  for (i in 1:(nrow(trading_df)-1)) {
    day_data <- trading_df[i, ]
    next_day_data <- trading_df[i+1, ]
    
    phi <- day_data[[ticker]] - predict(model, newdata = day_data)
    phi_t <- c(phi_t, as.numeric(phi))
    
    exposure <- sum(abs(sec_coef) * as.numeric(day_data[names(sec_coef)]))
    
    position_fraction <- -phi*(1-lambda) / (sigma_2 + (1-lambda)^2*phi^2) * exposure / 20
    position_fraction <- pmin(pmax(position_fraction, -0.9), 0.9)
    
    position_value <- position_fraction * tail(balance, 1)
    buy_ratio <- position_value / exposure
    buy_sec_amounts <- sec_coef * buy_ratio
    
    price_t0 <- sum(as.numeric(day_data[names(sec_coef)] * buy_sec_amounts))
    price_t1 <- sum(as.numeric(next_day_data[names(sec_coef)] * buy_sec_amounts))
    
    profit <- price_t1 - price_t0
    balance <- c(balance, tail(balance, 1) + profit)
  }
  
  data.frame(
    Date = data$Date,
    balance = balance,
    ticker = ticker
  )
}

# ---- Run for multiple tickers ----
tickers <- colnames(securities_wide %>% select(-Date))
balance_all <- map_dfr(tickers, ~ run_strategy(.x, securities_wide))

# ---- Plot ----
ggplot(balance_all, aes(x = Date, y = balance, color = ticker)) +
  geom_line(linewidth = 0.75) +
  labs(title = "Wealth over Time by Base",
       subtitle = "Starting at $1000 each | Training Data",
       y = NULL, x = NULL,
       color = "Base") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_date(date_breaks = "2 month", date_labels = "%b %Y") +
  theme_dark_mode()

```
This shows that the assumption of $\phi$ being an AR(1) is at least partially true, as well as confirming that the trade sizing found in the Formulation section is working.  
  
The following is the same chart as before, but isolating the XEM base, followed by XEM's price during the same period.
```{r}
ticker = "XEM"
# Put PHI and Price data in the same df
trading_df = securities_wide %>% 
  select(-Date)

starting_money = 1000
balance = c(starting_money)
phi_t = numeric()


model = auto_lm(trading_df, ticker, 0.1)
fit = Arima(model$residuals, order = c(1, 0, 0), include.mean = F)

sigma_2 = as.numeric(fit$sigma2)
lambda = as.numeric(fit$coef)
sec_coef = -model$coefficients# -1 because you solve you for the residual by subtracting lm
sec_coef[[ticker]] = 1
  
  
for (i in (1):(nrow(trading_df)-1)) { # -1 because you cant trade on last day
  
  day_data = trading_df[i, ]
  next_day_data = trading_df[i+1, ]
  
  phi = day_data[[ticker]] - predict(model, newdata = day_data)
  phi_t = c(phi_t, as.numeric(phi))
  
  exposure = sum(abs(sec_coef) * as.numeric(day_data[names(sec_coef)])) # abs() because cash is needed to hold short
  
  position_fraction = -phi*(1-lambda) / (sigma_2 + (1-lambda)^2*phi^2 ) * exposure / 20
  
  max_leverage <- 0.9  # cannot go beyond 90% of your balance
  position_fraction <- pmin(pmax(position_fraction, -max_leverage), max_leverage)


  position_value = position_fraction * tail(balance, 1)
  #print(position_fraction)
  
  buy_ratio = position_value / exposure
  
  buy_sec_amounts = sec_coef * buy_ratio
  
  price_t0 = 0
  for (j in names(sec_coef)) {
    price_t0 = price_t0 + as.numeric(day_data[j] * buy_sec_amounts[j])
  }
  price_t1 = 0
  for (j in names(sec_coef)) {
    price_t1 = price_t1 + as.numeric(next_day_data[j] * buy_sec_amounts[j])
  }
  
  profit = price_t1-price_t0
  balance = c(balance, tail(balance, 1) + profit)

}

balance_df = data.frame(
  Date = securities_wide$Date[(1):nrow(securities_wide)],
  balance = balance,
  phi = c(0, phi_t)
)


# Balance plot
p1 <- ggplot(balance_df, aes(x = Date, y = balance)) +
  geom_line(color = "green", linewidth = .75) +
  labs(title = "Wealth over Time", 
       subtitle = "Starting at $1000 with XEM Base | Training Data",
       y = NULL,
       x = NULL) +
  scale_y_continuous(breaks = seq(1000, 2250, 250), labels = scales::dollar) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b %Y") +
  theme_dark_mode()

# Phi plot
# p2 <- ggplot(balance_df, aes(x = Date, y = phi)) +
#   geom_line(color = "red") +
#   labs(title = "Phi over Time", y = "Phi") +
#   theme_dark_mode()
# 
p3 <- ggplot(securities_wide,
             aes(x = Date, y = .data[[ticker]])) + # / .data[[ticker]][1] * 1000 normalize if needed
  geom_line(color = "#999999", linewidth = .75) +
  labs(title = "XEM Price", y = NULL, x = NULL) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b %Y") +
  scale_y_continuous(labels = dollar) +
  theme_dark_mode()

# Print separately
p1
# p2
p3
```
XEM is a good example of this strategy's resilience to market conditions. Even when the price drops this strategy can continue to generate profit.  

  
### Testing Data
For a more realistic simulation a test-train-split of 80% training and 20% testing data is used. The following uses the first 80% of the data to find the weights $\gamma_i$ and the last 20% to find and trade $\phi$.   
The trade size will be divided by 20 and capping to $\pm0.9$ to avoid over leveraging.
```{r}

run_strategy <- function(ticker, train, test) {
  trading_df <- test %>% select(-Date)
  train_trading_df <- train %>% select(-Date)
  
  starting_money <- 1000
  balance <- c(starting_money)
  phi_t <- numeric()
  
  model <- auto_lm(train_trading_df, ticker, 0.05)
  fit <- Arima(model$residuals, order = c(1, 0, 0), include.mean = FALSE)
  
  sigma_2 <- as.numeric(fit$sigma2)
  lambda <- as.numeric(fit$coef)
  sec_coef <- -model$coefficients
  sec_coef[[ticker]] <- 1
  
  for (i in 1:(nrow(trading_df)-1)) {
    day_data <- trading_df[i, ]
    next_day_data <- trading_df[i+1, ]
    
    phi <- day_data[[ticker]] - predict(model, newdata = day_data)
    phi_t <- c(phi_t, as.numeric(phi))
    
    exposure <- sum(abs(sec_coef) * as.numeric(day_data[names(sec_coef)]))
    
    position_fraction <- -phi*(1-lambda) / (sigma_2 + (1-lambda)^2*phi^2) * exposure / 20
    position_fraction <- pmin(pmax(position_fraction, -0.9), 0.9)
    
    position_value <- position_fraction * tail(balance, 1)
    buy_ratio <- position_value / exposure
    buy_sec_amounts <- sec_coef * buy_ratio
    
    price_t0 <- sum(as.numeric(day_data[names(sec_coef)] * buy_sec_amounts))
    price_t1 <- sum(as.numeric(next_day_data[names(sec_coef)] * buy_sec_amounts))
    
    profit <- price_t1 - price_t0
    balance <- c(balance, tail(balance, 1) + profit)
  }
  
  data.frame(
    Date = test$Date,
    balance = balance,
    ticker = ticker
  )
}

# ---- Run for multiple tickers ----
tickers <- colnames(securities_wide %>% select(-Date))
balance_all <- map_dfr(tickers, ~ run_strategy(.x, train, test))

# ---- Plot ----
ggplot(balance_all, aes(x = Date, y = balance, color = ticker)) +
  geom_line(linewidth = 0.75) +
  labs(title = "Wealth over Time by Base",
       subtitle = "Starting at $1000 each | Using 80-20 split",
       y = NULL, x = NULL,
       color = "Base") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b %Y") +
  theme_dark_mode()
```
This is one of many ways to trade using this strategy. A sliding window can be used to find all $\gamma_i$ then trade on the very next day. Here is an example:
```{r}
run_strategy <- function(ticker, data, n_window = 60) {
  # Prepare data
  trading_df <- data %>%
    select(-Date) %>%
    dplyr::ungroup() %>%
    as.data.frame()
  
  if (as.integer(nrow(trading_df)) <= (n_window + 1)) {
    stop("Not enough rows in data for sliding window of size ", n_window)
  }

  starting_money <- 1000
  balance <- c(starting_money)
  phi_t <- numeric()

  # Loop over days starting after first n_window days
  for (i in (n_window+1):(nrow(trading_df)-1)) {
    # Sliding window of previous n_window days
    window_data <- trading_df[(i-n_window):(i-1), ]

    # Fit model on sliding window
    model <- auto_lm(window_data, ticker, 0.05)
    fit <- Arima(model$residuals, order = c(1, 0, 0), include.mean = FALSE)

    sigma_2 <- as.numeric(fit$sigma2)
    lambda <- as.numeric(fit$coef)
    sec_coef <- -model$coefficients
    sec_coef[[ticker]] <- 1

    # Data for today and next day
    day_data <- trading_df[i, ]
    next_day_data <- trading_df[i+1, ]

    # Compute residual
    phi <- day_data[[ticker]] - predict(model, newdata = day_data)
    phi_t <- c(phi_t, as.numeric(phi))

    exposure <- sum(abs(sec_coef) * as.numeric(day_data[names(sec_coef)]))

    position_fraction <- -phi*(1-lambda) / (sigma_2 + (1-lambda)^2*phi^2) * exposure / 20
    position_fraction <- pmin(pmax(position_fraction, -0.9), 0.9)

    position_value <- position_fraction * tail(balance, 1)
    buy_ratio <- position_value / exposure
    buy_sec_amounts <- sec_coef * buy_ratio

    price_t0 <- sum(as.numeric(day_data[names(sec_coef)] * buy_sec_amounts))
    price_t1 <- sum(as.numeric(next_day_data[names(sec_coef)] * buy_sec_amounts))

    profit <- price_t1 - price_t0
    balance <- c(balance, tail(balance, 1) + profit)
  }

  # Return only the dates starting from first traded day
  data.frame(
    Date = data$Date[(n_window+1):nrow(trading_df)],
    balance = balance,
    ticker = ticker
  )
}


# ---- Window sizes ----
window_sizes <- c(120, 90)
tickers <- colnames(securities_wide %>% select(-Date))

# ---- Run strategy for all tickers and all window sizes ----
balance_all <- map_dfr(window_sizes, function(w) {
  map_dfr(tickers, ~ run_strategy(.x, securities_wide, n_window = w)) %>%
    mutate(window = paste0(w, " day window"))
})

# ---- Plot with separate section for each window ----
ggplot(balance_all, aes(x = Date, y = balance, color = ticker)) +
  geom_line(linewidth = 0.75) +
  facet_wrap(~window, scales = "fixed") +
  labs(title = "Wealth over Time by Base",
       subtitle = "Starting at $1000 each  |  Sliding Window",
       y = NULL, x = NULL,
       color = "Base") +
  scale_y_continuous(labels = scales::dollar, breaks = seq(500, 2000, 250)) +
  scale_x_date(date_breaks = "2 month", date_labels = "%b %Y") +
  theme_dark_mode() +
  theme(
    strip.text.x = element_text(face = "bold", size = 12,
                                color = "grey80")
  )
  

```
While the strategy in it's current state doesn't perfectly translate to real-world data, there are a number of improvements that can be explored and implemented. Such improvements are explained in the Improvements section.


## Improvements

### Trading Multiple $\phi$
Many different bases are available. Calculating a unique $\phi$ for each and trading them simultaneously increases opportunities for profitable trades.

### Finding Security Weights with Gradient Descent
Currently, linear regression finds all $\gamma_i$ to perfectly fit the training data, but this leads to poor generalization. Gradient descent provides a better alternative. It supports weight decay to reduce overfitting and, through batching, adapts to recent patterns while still learning generalizable structures.

### Different Time Intervals
The backtesting performed in the Implementation tab used price data with a 1-day interval, but any interval (e.g., 1 minute, 1 hour, 1 week) could be applied. Each interval produces a slightly different $\phi$. Since $\phi_t = S_{B,t} - S_{P,t}$ and $S_B$ and $S_P$ often move together, subtracting them cancels overall trends and isolates their difference, $\phi$. Larger intervals consist only of the overall trends found in smaller intervals. Since smaller intervals do not contain these larger trends, the resulting $\phi$ values must differ.  
Trading across multiple intervals allows the strategy to exploit additional opportunities.

### Precise Trade Sizing
The trade sizing described in the Formulation tab used an approximation. Higher-order approximations could be used for greater accuracy, but an exact solution could be found numerically, maximizing returns.

### Different Bases
This strategy only uses securities as bases, but theoretically anything can be used as a base. For example, a linear combination of securities or constants (e.g., $S_B = 1$), though using non-tradable bases results in only pseudo-securities being traded.  
Using a wider set of bases creates more opportunities for profitable trades.

### Security Selection
To reduce overfitting, some securities are omitted based on p-values from a linear regression: any $\gamma_i$ with a p-value above a set threshold is removed, and the regression is recalculated until all remaining $\gamma_i$ fall below the threshold. However, p-values indicate the reliability of the estimate, not a security’s importance. As a result, useful securities may be excluded, suggesting the need for a more rigorous selection process.

### Trade Only Extreme $\phi$
The trade sizing already incorporates a version of this approach: the trade size scales linearly with the value of $\phi$. This method works to some extent, but small $\phi$ values yield an unfavorable risk-to-reward ratio. To address this, it may be advantageous to trade only when $\phi$ is large, perhaps one or two standard deviations above or below the mean.





:::